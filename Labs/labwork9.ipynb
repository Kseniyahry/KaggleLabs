{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":13311767,"datasetId":8386525,"databundleVersionId":14015016},{"sourceType":"datasetVersion","sourceId":13548016,"datasetId":8604370,"databundleVersionId":14274100},{"sourceType":"datasetVersion","sourceId":5219215,"datasetId":3036386,"databundleVersionId":5291708},{"sourceType":"datasetVersion","sourceId":3357494,"datasetId":2025491,"databundleVersionId":3408743},{"sourceType":"datasetVersion","sourceId":1517291,"datasetId":894296,"databundleVersionId":1551590},{"sourceType":"datasetVersion","sourceId":13925673,"datasetId":8873950,"databundleVersionId":14694712}],"dockerImageVersionId":30177,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Defines","metadata":{}},{"cell_type":"code","source":"BRAND = \"BMW\"\nIMG_SIZE = (256, 256)\nBATCH_SIZE = 32\nTOTAL_POSITIVE = 1000\nTOTAL_NEGATIVE = 1000\nSPLIT_RATIOS = {'train': 0.7, 'val': 0.15, 'test': 0.15}\n\nlogo_dir = '/kaggle/input/logo-dataset-2341-classes-and-167140-images/datasetcopy/trainandtest/train/Transportation/BMW'\nbackground_dir = '/kaggle/input/stanford-background-dataset/images'\ndataset_dir = '/kaggle/working/dataset'\nvideo_path = '/kaggle/input/lab6video/StockFootage.mp4'\nsave_model_dir = '/kaggle/working/model'\n\noutput_video = '/kaggle/working/output_with_boxes.mp4'\n\nprint('Logo dir:', logo_dir)\nprint('Background dir:', background_dir)\nprint('Dataset dir:', dataset_dir)\nprint('Video path:', video_path)\nprint('Save model dir:', save_model_dir)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T02:25:22.762716Z","iopub.execute_input":"2025-11-30T02:25:22.763505Z","iopub.status.idle":"2025-11-30T02:25:22.774716Z","shell.execute_reply.started":"2025-11-30T02:25:22.763446Z","shell.execute_reply":"2025-11-30T02:25:22.773933Z"}},"outputs":[{"name":"stdout","text":"Logo dir: /kaggle/input/logo-dataset-2341-classes-and-167140-images/datasetcopy/trainandtest/train/Transportation/BMW\nBackground dir: /kaggle/input/stanford-background-dataset/images\nDataset dir: /kaggle/working/dataset\nVideo path: /kaggle/input/lab6video/StockFootage.mp4\nSave model dir: /kaggle/working/model\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"!pip -q install ultralytics\n\nimport albumentations \nimport ultralytics, cv2, sys\nprint('ultralytics', ultralytics.__version__)\nprint('albumentations', albumentations.__version__)\nprint('opencv', cv2.__version__)\nprint('python', sys.version.split()[0])\n\nimport os\nos.environ[\"WANDB_MODE\"] = \"disabled\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T02:25:27.243339Z","iopub.execute_input":"2025-11-30T02:25:27.244026Z","iopub.status.idle":"2025-11-30T02:25:33.841151Z","shell.execute_reply.started":"2025-11-30T02:25:27.243989Z","shell.execute_reply":"2025-11-30T02:25:33.840286Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\nultralytics 8.0.145\nalbumentations 1.1.0\nopencv 4.12.0\npython 3.7.12\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## Dataset structure","metadata":{}},{"cell_type":"code","source":"import os, shutil, glob\nos.makedirs(dataset_dir, exist_ok=True)\nimages_dir = os.path.join(dataset_dir, 'images')\nlabels_dir = os.path.join(dataset_dir, 'labels')\nfor split in ['train','val','test']:\n    os.makedirs(os.path.join(images_dir, split), exist_ok=True)\n    os.makedirs(os.path.join(labels_dir, split), exist_ok=True)\n\nprint('Created dataset structure under', dataset_dir)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T02:25:37.376991Z","iopub.execute_input":"2025-11-30T02:25:37.377281Z","iopub.status.idle":"2025-11-30T02:25:37.384378Z","shell.execute_reply.started":"2025-11-30T02:25:37.377247Z","shell.execute_reply":"2025-11-30T02:25:37.383597Z"}},"outputs":[{"name":"stdout","text":"Created dataset structure under /kaggle/working/dataset\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## Dataset functions","metadata":{}},{"cell_type":"code","source":"import random, glob\nfrom PIL import Image\nimport numpy as np\nimport os\nfrom tqdm import tqdm\n\ndef load_random_logo():\n    files = glob.glob(os.path.join(logo_dir, '*'))\n    if not files:\n        raise FileNotFoundError('No logos found in ' + logo_dir)\n    return random.choice(files)\n\ndef load_random_bg():\n    files = glob.glob(os.path.join(background_dir, '*'))\n    if not files:\n        raise FileNotFoundError('No background images found in ' + background_dir)\n    return random.choice(files)\n\ndef place_logo_on_bg(logo_path, bg_path, out_size=IMG_SIZE, scale_range=(0.1,0.4), jitter=0.2):\n    bg = Image.open(bg_path).convert('RGBA')\n    logo = Image.open(logo_path).convert('RGBA')\n    bg = bg.resize(out_size, Image.LANCZOS)\n    scale = random.uniform(*scale_range)\n    logo_w = int(out_size[0] * scale)\n    w_ratio = logo_w / logo.width\n    logo_h = max(1, int(logo.height * w_ratio))\n    logo = logo.resize((logo_w, logo_h), Image.LANCZOS)\n    center_x = out_size[0] // 2 + int(random.uniform(-jitter, jitter) * out_size[0])\n    center_y = out_size[1] // 2 + int(random.uniform(-jitter, jitter) * out_size[1])\n    xmin = max(0, center_x - logo_w//2)\n    ymin = max(0, center_y - logo_h//2)\n    xmax = min(out_size[0], xmin + logo_w)\n    ymax = min(out_size[1], ymin + logo_h)\n    composed = Image.new('RGBA', out_size)\n    composed.paste(bg, (0,0))\n    composed.alpha_composite(logo, (xmin, ymin))\n    return composed.convert('RGB'), (xmin, ymin, xmax, ymax)\n\ndef xyxy_to_yolo_norm(bbox, img_w, img_h):\n    xmin, ymin, xmax, ymax = bbox\n    x_center = (xmin + xmax) / 2.0 / img_w\n    y_center = (ymin + ymax) / 2.0 / img_h\n    w = (xmax - xmin) / img_w\n    h = (ymax - ymin) / img_h\n    return x_center, y_center, w, h\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T02:25:40.577621Z","iopub.execute_input":"2025-11-30T02:25:40.578365Z","iopub.status.idle":"2025-11-30T02:25:40.590017Z","shell.execute_reply.started":"2025-11-30T02:25:40.578333Z","shell.execute_reply":"2025-11-30T02:25:40.589239Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"## Base dataset generation","metadata":{}},{"cell_type":"code","source":"import math, uuid\n\ndef generate_synthetic_dataset(n_pos=TOTAL_POSITIVE, n_neg=TOTAL_NEGATIVE, out_images_dir=images_dir, out_labels_dir=labels_dir):\n    random.seed(42)\n    for i in tqdm(range(n_pos), desc='Generating positives'):\n        logo_path = load_random_logo()\n        bg_path = load_random_bg()\n        img, bbox = place_logo_on_bg(logo_path, bg_path)\n        fname = f'pos_{i:05d}.jpg'\n        r = random.random()\n        if r < SPLIT_RATIOS['train']:\n            split = 'train'\n        elif r < SPLIT_RATIOS['train'] + SPLIT_RATIOS['val']:\n            split = 'val'\n        else:\n            split = 'test'\n        img_path = os.path.join(out_images_dir, split, fname)\n        os.makedirs(os.path.dirname(img_path), exist_ok=True)\n        img.save(img_path, quality=95)\n        lbl_path = os.path.join(out_labels_dir, split, os.path.splitext(fname)[0] + '.txt')\n        x_center, y_center, w, h = xyxy_to_yolo_norm(bbox, IMG_SIZE[0], IMG_SIZE[1])\n        with open(lbl_path, 'w') as f:\n            f.write(f\"0 {x_center:.6f} {y_center:.6f} {w:.6f} {h:.6f}\\n\")\n    for i in tqdm(range(n_neg), desc='Generating negatives'):\n        bg_path = load_random_bg()\n        bg = Image.open(bg_path).convert('RGB').resize(IMG_SIZE, Image.LANCZOS)\n        fname = f'neg_{i:05d}.jpg'\n        r = random.random()\n        if r < SPLIT_RATIOS['train']:\n            split = 'train'\n        elif r < SPLIT_RATIOS['train'] + SPLIT_RATIOS['val']:\n            split = 'val'\n        else:\n            split = 'test'\n        img_path = os.path.join(out_images_dir, split, fname)\n        os.makedirs(os.path.dirname(img_path), exist_ok=True)\n        bg.save(img_path, quality=95)\n        lbl_path = os.path.join(out_labels_dir, split, os.path.splitext(fname)[0] + '.txt')\n        open(lbl_path, 'w').close()\n\ngenerate_synthetic_dataset(n_pos=TOTAL_POSITIVE, n_neg=TOTAL_NEGATIVE)\nprint('Synthetic dataset created.')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T02:25:43.908672Z","iopub.execute_input":"2025-11-30T02:25:43.909509Z","iopub.status.idle":"2025-11-30T02:26:09.889061Z","shell.execute_reply.started":"2025-11-30T02:25:43.909474Z","shell.execute_reply":"2025-11-30T02:26:09.888303Z"}},"outputs":[{"name":"stderr","text":"Generating positives: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:15<00:00, 62.79it/s]\nGenerating negatives: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:10<00:00, 99.64it/s]","output_type":"stream"},{"name":"stdout","text":"Synthetic dataset created.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"## Dataset augmentation via albumentations","metadata":{}},{"cell_type":"code","source":"from albumentations import (Compose, HorizontalFlip, VerticalFlip, RandomRotate90, ShiftScaleRotate,\n                            RandomBrightnessContrast, Blur, GaussNoise, CLAHE, HueSaturationValue,\n                            Cutout, RandomShadow)\nfrom albumentations import BboxParams\nimport cv2\n\ntransform = Compose([\n    HorizontalFlip(p=0.5),\n    VerticalFlip(p=0.1),\n    RandomRotate90(p=0.3),\n    ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=20, p=0.5, border_mode=0),\n    RandomBrightnessContrast(p=0.5),\n    HueSaturationValue(p=0.4),\n    Cutout(num_holes=8, max_h_size=32, max_w_size=32, p=0.3),\n    Blur(blur_limit=3, p=0.2),\n    GaussNoise(var_limit=(10.0, 50.0), p=0.2),\n    RandomShadow(p=0.2)\n], bbox_params=BboxParams(format='yolo', label_fields=['category_ids']))\n\ndef augment_and_save_all(n_augs=2):\n    for split in ['train','val']:\n        img_files = glob.glob(os.path.join(images_dir, split, '*.jpg'))\n        for img_path in tqdm(img_files, desc=f'Augment {split}'):\n            base = os.path.splitext(os.path.basename(img_path))[0]\n            lbl_path = os.path.join(labels_dir, split, base + '.txt')\n            boxes = []\n            cats = []\n            if os.path.exists(lbl_path):\n                with open(lbl_path, 'r') as f:\n                    for line in f:\n                        parts = line.strip().split()\n                        if len(parts) != 5:\n                            continue\n                        cls = int(parts[0])\n                        xc, yc, bw, bh = map(float, parts[1:5])\n                        boxes.append([xc, yc, bw, bh])\n                        cats.append(cls)\n            img = cv2.imread(img_path)\n            h,w = img.shape[:2]\n            for i in range(n_augs):\n                try:\n                    augmented = transform(image=img, bboxes=boxes, category_ids=cats)\n                    aug_img = augmented['image']\n                    aug_boxes = augmented['bboxes']\n                    aug_cats = augmented['category_ids']\n                    if len(aug_boxes) == 0 and len(boxes)>0:\n                        continue\n                    out_base = f\"{base}_aug{i}\"\n                    out_img_path = os.path.join(images_dir, split, out_base + '.jpg')\n                    out_lbl_path = os.path.join(labels_dir, split, out_base + '.txt')\n                    cv2.imwrite(out_img_path, aug_img)\n                    with open(out_lbl_path, 'w') as f:\n                        for cls, b in zip(aug_cats, aug_boxes):\n                            f.write(f\"{cls} {b[0]:.6f} {b[1]:.6f} {b[2]:.6f} {b[3]:.6f}\\n\")\n                except:\n                    pass\n    print('Augmentation finished.')\n\naugment_and_save_all()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T02:26:13.963459Z","iopub.execute_input":"2025-11-30T02:26:13.963692Z","iopub.status.idle":"2025-11-30T02:26:55.580229Z","shell.execute_reply.started":"2025-11-30T02:26:13.963667Z","shell.execute_reply":"2025-11-30T02:26:55.579460Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/albumentations/augmentations/transforms.py:691: FutureWarning: This class has been deprecated. Please use CoarseDropout\n  FutureWarning,\nAugment train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4143/4143 [00:33<00:00, 122.68it/s]\nAugment val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 981/981 [00:07<00:00, 125.60it/s]","output_type":"stream"},{"name":"stdout","text":"Augmentation finished.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"## data.yaml for Ultralytics","metadata":{}},{"cell_type":"code","source":"data_yaml = {\n    'train': os.path.join(dataset_dir, 'images', 'train'),\n    'val': os.path.join(dataset_dir, 'images', 'val'),\n    'nc': 1,\n    'names': ['logo']\n}\nimport yaml\nwith open(os.path.join(dataset_dir, 'data.yaml'), 'w') as f:\n    yaml.dump(data_yaml, f)\nprint('Wrote', os.path.join(dataset_dir, 'data.yaml'))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T02:26:58.408842Z","iopub.execute_input":"2025-11-30T02:26:58.409114Z","iopub.status.idle":"2025-11-30T02:26:58.416077Z","shell.execute_reply.started":"2025-11-30T02:26:58.409081Z","shell.execute_reply":"2025-11-30T02:26:58.415357Z"}},"outputs":[{"name":"stdout","text":"Wrote /kaggle/working/dataset/data.yaml\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"## Model training","metadata":{}},{"cell_type":"code","source":"from ultralytics import YOLO\nos.makedirs(save_model_dir, exist_ok=True)\nmodel = YOLO(\"/kaggle/input/yolo11/yolov8n.pt\") \nmodel.train(data=os.path.join(dataset_dir, 'data.yaml'), epochs=6, imgsz=640, batch=BATCH_SIZE//2 if BATCH_SIZE>=8 else 8, name='mkr_logo_exp', project=\"yolo\")\nprint('Training launched. Check runs folder in', save_model_dir)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T02:27:02.022536Z","iopub.execute_input":"2025-11-30T02:27:02.023280Z","iopub.status.idle":"2025-11-30T02:40:41.402405Z","shell.execute_reply.started":"2025-11-30T02:27:02.023240Z","shell.execute_reply":"2025-11-30T02:40:41.401428Z"}},"outputs":[{"name":"stderr","text":"New https://pypi.org/project/ultralytics/8.3.233 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\nUltralytics YOLOv8.0.145 ðŸš€ Python-3.7.12 torch-1.9.1 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\nWARNING âš ï¸ Upgrade to torch>=2.0.0 for deterministic training.\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/kaggle/input/yolo11/yolov8n.pt, data=/kaggle/working/dataset/data.yaml, epochs=6, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=yolo, name=mkr_logo_exp, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=yolo/mkr_logo_exp3\nOverriding model.yaml nc=80 with nc=1\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \nModel summary: 225 layers, 3011043 parameters, 3011027 gradients\n\nTransferred 319/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir yolo/mkr_logo_exp3', view at http://localhost:6006/\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/dataset/labels/train... 9667 images, 4823 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9667/9667 [00:05<00:00, 1705.52it/s]\n\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /kaggle/working/dataset/labels/train.cache\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/dataset/labels/val... 2287 images, 1057 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2287/2287 [00:40<00:00, 55.96it/s]  \n\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /kaggle/working/dataset/labels/val.cache\nPlotting labels to yolo/mkr_logo_exp3/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1myolo/mkr_logo_exp3\u001b[0m\nStarting training for 6 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n  0%|          | 0/605 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ultralytics/engine/trainer.py:461: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n  torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=10.0)  # clip gradients\n        1/6      2.71G     0.7913      1.661      1.107          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 605/605 [01:58<00:00,  5.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72/72 [00:12<00:00,  5.77it/s]\n                   all       2287       1230      0.903      0.811      0.883      0.663\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n        2/6      2.59G     0.8024     0.8542      1.119          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 605/605 [01:51<00:00,  5.42it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72/72 [00:12<00:00,  5.80it/s]\n                   all       2287       1230      0.942      0.907      0.961      0.814\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n        3/6      2.58G      0.726     0.7008      1.092          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 605/605 [01:49<00:00,  5.55it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72/72 [00:12<00:00,  5.76it/s]\n                   all       2287       1230      0.965      0.915      0.955      0.825\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n        4/6      2.58G     0.6573     0.6162      1.056          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 605/605 [01:47<00:00,  5.60it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72/72 [00:12<00:00,  5.94it/s]\n                   all       2287       1230       0.98      0.953      0.978      0.871\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n        5/6      2.58G     0.6228     0.5628      1.036          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 605/605 [01:47<00:00,  5.61it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72/72 [00:12<00:00,  5.91it/s]\n                   all       2287       1230      0.975      0.968       0.98      0.879\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n        6/6      2.58G     0.5565     0.4868      1.017          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 605/605 [01:47<00:00,  5.61it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72/72 [00:14<00:00,  4.94it/s]\n                   all       2287       1230      0.975      0.966      0.984      0.907\n\n6 epochs completed in 0.208 hours.\nOptimizer stripped from yolo/mkr_logo_exp3/weights/last.pt, 6.2MB\nOptimizer stripped from yolo/mkr_logo_exp3/weights/best.pt, 6.2MB\n\nValidating yolo/mkr_logo_exp3/weights/best.pt...\nUltralytics YOLOv8.0.145 ðŸš€ Python-3.7.12 torch-1.9.1 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\nModel summary (fused): 168 layers, 3005843 parameters, 0 gradients\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72/72 [00:14<00:00,  4.92it/s]\n                   all       2287       1230      0.975      0.966      0.984      0.907\nSpeed: 0.3ms preprocess, 2.8ms inference, 0.0ms loss, 0.7ms postprocess per image\nResults saved to \u001b[1myolo/mkr_logo_exp3\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Training launched. Check runs folder in /kaggle/working/model\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## Video inference","metadata":{}},{"cell_type":"code","source":"import cv2\nfrom ultralytics import YOLO\ntrained_weights = None\npossible = sorted(glob.glob(os.path.join(save_model_dir, 'runs', 'train', 'mkr_logo_exp', 'weights', '*.pt')))\nif possible:\n    trained_weights = possible[-1]\nelse:\n    trained_weights = '/kaggle/input/yolo11/yolov8n.pt'\n\nprint('Using weights:', trained_weights)\nmodel = YOLO(trained_weights)\n\ncap = cv2.VideoCapture(video_path)\nif not cap.isOpened():\n    raise FileNotFoundError('Cannot open video: ' + video_path)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nfps = cap.get(cv2.CAP_PROP_FPS) or 30.0\nw = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nh = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nwriter = cv2.VideoWriter(output_video, fourcc, fps, (w,h))\n\nfrom tqdm import tqdm\nframe_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\nfor _ in tqdm(range(frame_count), desc='Processing video'):\n    ret, frame = cap.read()\n    if not ret:\n        break\n    results = model.predict(frame, imgsz=640, verbose=False)\n    try:\n        res = results[0]\n        boxes = res.boxes\n        for box in boxes:\n            xyxy = box.xyxy[0].cpu().numpy()\n            conf = float(box.conf[0]) if hasattr(box, 'conf') else 0.0\n            cls = int(box.cls[0]) if hasattr(box, 'cls') else 0\n            label = f\"{BRAND}:{conf:.2f}\"\n            x1,y1,x2,y2 = map(int, xyxy)\n            cv2.rectangle(frame, (x1,y1), (x2,y2), (0,255,0), 2)\n            cv2.putText(frame, label, (x1, max(15,y1-5)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2)\n    except Exception:\n        pass\n    writer.write(frame)\n\ncap.release()\nwriter.release()\nprint('Saved output video to', output_video)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T02:41:08.722200Z","iopub.execute_input":"2025-11-30T02:41:08.722485Z","iopub.status.idle":"2025-11-30T02:41:16.324899Z","shell.execute_reply.started":"2025-11-30T02:41:08.722448Z","shell.execute_reply":"2025-11-30T02:41:16.324126Z"}},"outputs":[{"name":"stdout","text":"Using weights: /kaggle/input/yolo11/yolov8n.pt\n","output_type":"stream"},{"name":"stderr","text":"Processing video: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 622/622 [00:07<00:00, 82.84it/s]","output_type":"stream"},{"name":"stdout","text":"Saved output video to /kaggle/working/output_with_boxes.mp4\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":20}]}